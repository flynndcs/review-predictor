{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing, load_wine, make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV, validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Machine Learning Model Selection Tutorial\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "## Part 1: Understanding Cross-Validation\n",
    "print(\"\\nüìä PART 1: CROSS-VALIDATION FUNDAMENTALS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load a real dataset - California Housing (regression problem)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "california = fetch_california_housing()\n",
    "X_california, y_california = california.data, california.target\n",
    "feature_names = california.feature_names\n",
    "\n",
    "print(f\"Dataset: California Housing Prices\")\n",
    "print(f\"Features: {len(feature_names)} ({', '.join(feature_names[:5])}...)\")\n",
    "print(f\"Samples: {len(X_california)}\")\n",
    "print(f\"Target: Median house value in hundreds of thousands of dollars\")\n",
    "\n",
    "# Create a simple model to demonstrate CV\n",
    "rf_basic = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Traditional Train-Test Split (what NOT to rely on alone)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_california, y_california, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "rf_basic.fit(X_train, y_train)\n",
    "single_score = rf_basic.score(X_test, y_test)\n",
    "print(f\"\\n‚ùå Single train-test split R¬≤ score: {single_score:.4f}\")\n",
    "print(\"Problem: This gives us only ONE estimate of performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. K-Fold Cross-Validation - The Better Way\n",
    "print(f\"\\n‚úÖ K-Fold Cross-Validation (k=5):\")\n",
    "cv_scores = cross_val_score(rf_basic, X_california, y_california, cv=5, scoring='r2')\n",
    "print(f\"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "print(\"Benefit: More robust estimate with confidence interval!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Different CV Strategies Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Cross-Validation Strategies Comparison', fontsize=16)\n",
    "\n",
    "# Compare different k values\n",
    "k_values = [3, 5, 10, 20]\n",
    "cv_means = []\n",
    "cv_stds = []\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    scores = cross_val_score(rf_basic, X_california, y_california, cv=k, scoring='r2')\n",
    "    cv_means.append(scores.mean())\n",
    "    cv_stds.append(scores.std())\n",
    "    \n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.bar(range(len(scores)), scores, alpha=0.7)\n",
    "    ax.axhline(y=scores.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {scores.mean():.3f}')\n",
    "    ax.set_title(f'{k}-Fold CV (std: {scores.std():.3f})')\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('R¬≤ Score')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of CV strategy impact\n",
    "cv_comparison = pd.DataFrame({\n",
    "    'K_Folds': k_values,\n",
    "    'Mean_Score': cv_means,\n",
    "    'Std_Dev': cv_stds,\n",
    "    'CI_Width': [2*std for std in cv_stds]\n",
    "})\n",
    "print(\"\\nüìà Cross-Validation Strategy Comparison:\")\n",
    "print(cv_comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Hyperparameter Tuning\n",
    "print(\"\\n\\nüîß PART 2: HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load classification dataset for hyperparameter tuning\n",
    "wine = load_wine()\n",
    "X_wine, y_wine = wine.data, wine.target\n",
    "\n",
    "print(\"Dataset: Wine Classification\")\n",
    "print(f\"Features: {X_wine.shape[1]}, Samples: {X_wine.shape[0]}, Classes: {len(np.unique(y_wine))}\")\n",
    "\n",
    "# Scale features for SVM (important!)\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Manual Hyperparameter Testing (inefficient way)\n",
    "print(\"\\n‚ùå Manual Hyperparameter Testing:\")\n",
    "manual_results = []\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "gamma_values = ['scale', 'auto', 0.01, 0.1]\n",
    "\n",
    "for C in C_values[:2]:  # Limited for demo\n",
    "    for gamma in gamma_values[:2]:\n",
    "        svm = SVC(C=C, gamma=gamma, random_state=42)\n",
    "        scores = cross_val_score(svm, X_wine_scaled, y_wine, cv=5)\n",
    "        manual_results.append({\n",
    "            'C': C, 'gamma': gamma, 'mean_score': scores.mean(), 'std': scores.std()\n",
    "        })\n",
    "        print(f\"C={C}, gamma={gamma}: {scores.mean():.4f} (¬±{scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Grid Search - Systematic Approach\n",
    "print(f\"\\n‚úÖ Grid Search Cross-Validation:\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42), \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU corebs\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_wine_scaled, y_wine)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Total combinations tested: {len(grid_search.cv_results_['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Randomized Search - Efficient Alternative\n",
    "print(f\"\\n‚ö° Randomized Search (faster alternative):\")\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),  # Continuous distribution\n",
    "    'gamma': uniform(0.001, 1),\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=50,  # Number of parameter combinations to try\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_wine_scaled, y_wine)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Combinations tested: {random_search.n_iter} (vs {len(grid_search.cv_results_['params'])} in Grid Search)\")\n",
    "\n",
    "# Comparison of methods\n",
    "tuning_comparison = pd.DataFrame({\n",
    "    'Method': ['Manual', 'Grid Search', 'Random Search'],\n",
    "    'Best_Score': [\n",
    "        max([r['mean_score'] for r in manual_results]),\n",
    "        grid_search.best_score_,\n",
    "        random_search.best_score_\n",
    "    ],\n",
    "    'Combinations_Tested': [4, len(grid_search.cv_results_['params']), 50],\n",
    "    'Time_Efficiency': ['Low', 'Low', 'High']\n",
    "})\n",
    "print(f\"\\nüìä Hyperparameter Tuning Comparison:\")\n",
    "print(tuning_comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: Model Selection\n",
    "print(\"\\n\\nüèÜ PART 3: MODEL SELECTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a more complex dataset for model comparison\n",
    "X_complex, y_complex = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, \n",
    "    n_redundant=5, n_classes=3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Dataset: Complex Multi-class Classification\")\n",
    "print(f\"Features: {X_complex.shape[1]}, Samples: {X_complex.shape[0]}, Classes: 3\")\n",
    "\n",
    "# Define multiple models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Model Comparison\n",
    "print(f\"\\nüîç Basic Model Comparison (5-Fold CV):\")\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'SVM':\n",
    "        # Scale features for SVM\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_complex)\n",
    "        scores = cross_val_score(model, X_scaled, y_complex, cv=5)\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_complex, y_complex, cv=5)\n",
    "    \n",
    "    model_results[name] = scores\n",
    "    print(f\"{name:20}: {scores.mean():.4f} (¬±{scores.std():.4f})\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(model_results.keys())\n",
    "means = [model_results[name].mean() for name in model_names]\n",
    "stds = [model_results[name].std() for name in model_names]\n",
    "\n",
    "bars = plt.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n",
    "             f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Advanced Model Selection with Hyperparameter Tuning\n",
    "print(f\"\\nüöÄ Advanced Model Selection with Hyperparameter Tuning:\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "tuned_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        \n",
    "        if name == 'SVM':\n",
    "            X_model = StandardScaler().fit_transform(X_complex)\n",
    "        else:\n",
    "            X_model = X_complex\n",
    "            \n",
    "        grid = GridSearchCV(\n",
    "            model, param_grids[name], cv=5, \n",
    "            scoring='accuracy', n_jobs=-1\n",
    "        )\n",
    "        grid.fit(X_model, y_complex)\n",
    "        \n",
    "        best_models[name] = grid.best_estimator_\n",
    "        tuned_results[name] = grid.best_score_\n",
    "        \n",
    "        print(f\"Best params: {grid.best_params_}\")\n",
    "        print(f\"Best score: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèÖ FINAL MODEL RANKINGS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Combine basic and tuned results\n",
    "final_scores = {}\n",
    "for name in models.keys():\n",
    "    if name in tuned_results:\n",
    "        final_scores[name] = tuned_results[name]\n",
    "    else:\n",
    "        final_scores[name] = model_results[name].mean()\n",
    "\n",
    "# Sort by performance\n",
    "sorted_models = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (name, score) in enumerate(sorted_models, 1):\n",
    "    status = \"‚≠ê WINNER!\" if i == 1 else f\"#{i}\"\n",
    "    tuned_status = \"(Tuned)\" if name in tuned_results else \"(Basic)\"\n",
    "    print(f\"{status:12} {name:20} {tuned_status:8}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 4: Practical Validation Strategies\n",
    "print(f\"\\n\\n‚úÖ PART 4: PRACTICAL VALIDATION STRATEGIES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Learning Curves - Diagnose overfitting/underfitting\n",
    "best_model_name = sorted_models[0][0]\n",
    "best_model = best_models.get(best_model_name, models[best_model_name])\n",
    "\n",
    "print(f\"Analyzing learning curves for: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'SVM':\n",
    "    X_analysis = StandardScaler().fit_transform(X_complex)\n",
    "else:\n",
    "    X_analysis = X_complex\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_analysis, y_complex, cv=5, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                 alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                 alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title(f'Learning Curves - {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "gap = train_mean[-1] - val_mean[-1]\n",
    "if gap > 0.05:\n",
    "    print(f\"‚ö†Ô∏è  Large gap ({gap:.3f}) suggests overfitting\")\n",
    "    print(\"   Recommendation: Regularization, more data, or simpler model\")\n",
    "elif val_mean[-1] < 0.8:\n",
    "    print(f\"üìà Low validation score suggests underfitting\")\n",
    "    print(\"   Recommendation: More complex model or feature engineering\")\n",
    "else:\n",
    "    print(f\"‚úÖ Good balance - training/validation gap: {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Validation Curve - Analyze single hyperparameter\n",
    "print(f\"\\nüìà Validation Curve Analysis:\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_name = 'n_estimators'\n",
    "    param_range = [10, 50, 100, 200, 500]\n",
    "elif best_model_name == 'SVM':\n",
    "    param_name = 'C'\n",
    "    param_range = [0.01, 0.1, 1, 10, 100]\n",
    "else:\n",
    "    param_name = 'max_depth'\n",
    "    param_range = [1, 3, 5, 10, 15, None]\n",
    "\n",
    "if param_range[-1] is not None:  # Avoid None in validation curve for some models\n",
    "    train_scores, val_scores = validation_curve(\n",
    "        models[best_model_name], X_analysis, y_complex, \n",
    "        param_name=param_name, param_range=param_range[:-1], \n",
    "        cv=5, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    \n",
    "    plt.plot(param_range[:-1], train_mean, 'o-', label='Training Score')\n",
    "    plt.plot(param_range[:-1], val_mean, 'o-', label='Validation Score')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title(f'Validation Curve - {param_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if param_name == 'C':\n",
    "        plt.xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 5: Real-World Application Checklist\n",
    "print(f\"\\n\\nüìã PART 5: REAL-WORLD APPLICATION CHECKLIST\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "checklist = \"\"\"\n",
    "‚úÖ DATA PREPARATION:\n",
    "   ‚ñ° Handle missing values appropriately\n",
    "   ‚ñ° Scale features when needed (SVM, Neural Networks)\n",
    "   ‚ñ° Check for data leakage\n",
    "   ‚ñ° Ensure proper train/validation/test splits\n",
    "\n",
    "‚úÖ CROSS-VALIDATION STRATEGY:\n",
    "   ‚ñ° Use StratifiedKFold for imbalanced classification\n",
    "   ‚ñ° Use TimeSeriesSplit for temporal data\n",
    "   ‚ñ° Choose appropriate k (5-10 typically good)\n",
    "   ‚ñ° Consider computational constraints\n",
    "\n",
    "‚úÖ HYPERPARAMETER TUNING:\n",
    "   ‚ñ° Start with RandomizedSearchCV for efficiency\n",
    "   ‚ñ° Use GridSearchCV for final fine-tuning\n",
    "   ‚ñ° Set up proper parameter ranges\n",
    "   ‚ñ° Use nested CV for unbiased evaluation\n",
    "\n",
    "‚úÖ MODEL SELECTION:\n",
    "   ‚ñ° Compare multiple algorithm families\n",
    "   ‚ñ° Consider interpretability requirements\n",
    "   ‚ñ° Evaluate computational constraints\n",
    "   ‚ñ° Check learning curves for overfitting\n",
    "\n",
    "‚úÖ VALIDATION:\n",
    "   ‚ñ° Use holdout test set for final evaluation\n",
    "   ‚ñ° Analyze confusion matrices for classification\n",
    "   ‚ñ° Check residual plots for regression\n",
    "   ‚ñ° Validate on similar but different datasets if possible\n",
    "\n",
    "‚úÖ PRODUCTION CONSIDERATIONS:\n",
    "   ‚ñ° Model versioning and reproducibility\n",
    "   ‚ñ° Performance monitoring setup\n",
    "   ‚ñ° Retrain schedule planning\n",
    "   ‚ñ° Feature drift detection\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\nüéØ KEY TAKEAWAYS:\")\n",
    "print(\"-\" * 20)\n",
    "print(\"1. Always use cross-validation - never trust a single train-test split\")\n",
    "print(\"2. Start with RandomizedSearchCV, then fine-tune with GridSearchCV\")\n",
    "print(\"3. Compare multiple model types before settling on one\")\n",
    "print(\"4. Use learning curves to diagnose overfitting/underfitting\")\n",
    "print(\"5. Keep a holdout test set that you NEVER touch during development\")\n",
    "print(\"6. Consider real-world constraints (speed, interpretability, maintenance)\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"- Apply these techniques to your own dataset\")\n",
    "print(\"- Experiment with ensemble methods (Voting, Stacking)\")  \n",
    "print(\"- Learn about advanced CV strategies (Nested CV, Group CV)\")\n",
    "print(\"- Explore automated ML libraries (Auto-sklearn, TPOT)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Tutorial Complete! Happy Machine Learning! ü§ñ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
