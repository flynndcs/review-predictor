{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Environment ready! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the dataset\n",
    "print(\"Loading Amazon Reviews dataset...\")\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "                      \"raw_review_All_Beauty\", \n",
    "                      trust_remote_code=True)\n",
    "\n",
    "df = dataset['full'].to_pandas()\n",
    "print(f\"Loaded {len(df)} reviews!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Quick exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nRating distribution:\")\n",
    "df['rating'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words (basic analysis)\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert to lowercase, remove special chars, split\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    return text\n",
    "\n",
    "# Sample analysis on first 1000 reviews for speed\n",
    "sample_df = df.head(1000).copy()\n",
    "sample_df['clean_text'] = sample_df['text'].apply(clean_text_basic)\n",
    "\n",
    "# Most common words\n",
    "all_words = []\n",
    "for text in sample_df['clean_text']:\n",
    "    all_words.extend(text.split())\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "print(\"Most common words:\")\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential issues\n",
    "print(\"Data Quality Checks:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "# 1. Missing text or ratings\n",
    "print(f\"Reviews with missing text: {df['text'].isnull().sum()}\")\n",
    "print(f\"Reviews with missing ratings: {df['rating'].isnull().sum()}\")\n",
    "\n",
    "# 2. Invalid ratings\n",
    "valid_ratings = df['rating'].between(1, 5)\n",
    "print(f\"Invalid ratings: {(~valid_ratings).sum()}\")\n",
    "\n",
    "# 3. Duplicate reviews\n",
    "duplicates = df.duplicated(subset=['text'], keep=False)\n",
    "print(f\"Potential duplicate reviews: {duplicates.sum()}\")\n",
    "\n",
    "# 4. Empty or very short reviews\n",
    "empty_reviews = df['text'].str.strip().str.len() < 10\n",
    "print(f\"Very short reviews (< 10 chars): {empty_reviews.sum()}\")\n",
    "\n",
    "# 5. Extremely long reviews (might be spam)\n",
    "very_long = df['text_length'] > 5000\n",
    "print(f\"Extremely long reviews (> 5000 chars): {very_long.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at examples by rating\n",
    "print(\"\\nSample 1-star reviews:\")\n",
    "one_star = df[df['rating'] == 1.0]['text'].head(3)\n",
    "for i, review in enumerate(one_star, 1):\n",
    "    print(f\"{i}. {review[:200]}...\")\n",
    "\n",
    "print(\"\\nSample 5-star reviews:\")\n",
    "five_star = df[df['rating'] == 5.0]['text'].head(3)\n",
    "for i, review in enumerate(five_star, 1):\n",
    "    print(f\"{i}. {review[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new notebook cell or section\n",
    "print(\"ML Problem Definition:\")\n",
    "print(\"Input: Review text\")\n",
    "print(\"Output: Star rating (1-5)\")\n",
    "print(\"Task: Multi-class classification\")\n",
    "\n",
    "# Check the target distribution\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "rating_dist = df['rating'].value_counts().sort_index()\n",
    "print(rating_dist)\n",
    "print(f\"\\nClass balance: {(rating_dist / len(df) * 100).round(1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, let's work with a manageable subset for development\n",
    "# Start with 10k samples, scale up later\n",
    "sample_size = min(10000, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Working with {len(df_sample)} samples for initial development\")\n",
    "\n",
    "# Create features and target\n",
    "X = df_sample['text'].copy()\n",
    "y = df_sample['rating'].copy()\n",
    "\n",
    "# Split the data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 â‰ˆ 0.15\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\") \n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "for split_name, split_y in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    dist = split_y.value_counts(normalize=True).sort_index() * 100\n",
    "    print(f\"{split_name}: {dict(dist.round(1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Basic text preprocessing function\n",
    "    Start simple, add complexity as needed\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Optional: Remove URLs, emails, etc.\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "X_train_clean = X_train.apply(preprocess_text)\n",
    "X_val_clean = X_val.apply(preprocess_text)\n",
    "X_test_clean = X_test.apply(preprocess_text)\n",
    "\n",
    "print(\"Sample preprocessed texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {X_train.iloc[i][:100]}...\")\n",
    "    print(f\"Cleaned:  {X_train_clean.iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TF-IDF vectorizer for baseline model\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # Start with 5k features\n",
    "    stop_words='english',   # Remove common words\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    min_df=2,               # Ignore terms appearing in < 2 documents\n",
    "    max_df=0.95             # Ignore terms appearing in > 95% of documents\n",
    ")\n",
    "\n",
    "# Fit only on training data\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_vectors = vectorizer.fit_transform(X_train_clean)\n",
    "X_val_vectors = vectorizer.transform(X_val_clean)\n",
    "X_test_vectors = vectorizer.transform(X_test_clean)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_vectors.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Look at some features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For most algorithms, we can use ratings directly\n",
    "# But let's also prepare a binary version for experimentation\n",
    "\n",
    "# Multi-class target (1-5 stars)\n",
    "y_train_multi = y_train.copy()\n",
    "y_val_multi = y_val.copy()\n",
    "y_test_multi = y_test.copy()\n",
    "\n",
    "# Binary target (positive vs negative sentiment)\n",
    "# 1-2 stars = negative (0), 4-5 stars = positive (1), ignore 3 stars\n",
    "def convert_to_binary(rating):\n",
    "    if rating <= 2:\n",
    "        return 0  # Negative\n",
    "    elif rating >= 4:\n",
    "        return 1  # Positive\n",
    "    else:\n",
    "        return None  # Neutral - we'll filter these out\n",
    "\n",
    "# Create binary datasets\n",
    "binary_mask_train = y_train.apply(lambda x: convert_to_binary(x) is not None).values\n",
    "binary_mask_val = y_val.apply(lambda x: convert_to_binary(x) is not None).values\n",
    "binary_mask_test = y_test.apply(lambda x: convert_to_binary(x) is not None).values\n",
    "\n",
    "y_train_binary = y_train[y_train.apply(lambda x: convert_to_binary(x) is not None)].apply(convert_to_binary)\n",
    "y_val_binary = y_val[y_val.apply(lambda x: convert_to_binary(x) is not None)].apply(convert_to_binary)\n",
    "y_test_binary = y_test[y_test.apply(lambda x: convert_to_binary(x) is not None)].apply(convert_to_binary)\n",
    "\n",
    "X_train_vectors_binary = X_train_vectors[binary_mask_train]\n",
    "X_val_vectors_binary = X_val_vectors[binary_mask_val]\n",
    "X_test_vectors_binary = X_test_vectors[binary_mask_test]\n",
    "\n",
    "print(f\"Multi-class dataset: {len(y_train_multi)} train samples\")\n",
    "print(f\"Binary dataset: {len(y_train_binary)} train samples\")\n",
    "print(f\"Binary class distribution: {y_train_binary.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a data directory\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Save the preprocessed data\n",
    "data_to_save = {\n",
    "    'X_train_vectors': X_train_vectors,\n",
    "    'X_val_vectors': X_val_vectors,\n",
    "    'X_test_vectors': X_test_vectors,\n",
    "    'y_train_multi': y_train_multi,\n",
    "    'y_val_multi': y_val_multi,\n",
    "    'y_test_multi': y_test_multi,\n",
    "    'X_train_vectors_binary': X_train_vectors_binary,\n",
    "    'X_val_vectors_binary': X_val_vectors_binary,\n",
    "    'X_test_vectors_binary': X_test_vectors_binary,\n",
    "    'y_train_binary': y_train_binary,\n",
    "    'y_val_binary': y_val_binary,\n",
    "    'y_test_binary': y_test_binary,\n",
    "    'vectorizer': vectorizer,\n",
    "    'original_text_train': X_train,\n",
    "    'original_text_val': X_val,\n",
    "    'original_text_test': X_test\n",
    "}\n",
    "\n",
    "with open('data/processed/preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "print(\"Preprocessed data saved to 'data/processed/preprocessed_data.pkl'\")\n",
    "print(\"\\nReady for model training! ðŸŽ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Start with binary classification (easier to interpret)\n",
    "print(\"Training Logistic Regression for Binary Classification (Positive vs Negative)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize the model\n",
    "lr_binary = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,  # Increase iterations for convergence\n",
    "    C=1.0           # Regularization strength (lower = more regularization)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training...\")\n",
    "lr_binary.fit(X_train_vectors_binary, y_train_binary)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred_binary = lr_binary.predict(X_val_vectors_binary)\n",
    "y_val_prob_binary = lr_binary.predict_proba(X_val_vectors_binary)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_val_binary, y_val_pred_binary)\n",
    "print(f\"Validation Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val_binary, y_val_pred_binary, \n",
    "                          target_names=['Negative (1-2â˜…)', 'Positive (4-5â˜…)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = confusion_matrix(y_val_binary, y_val_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Prediction Confidence Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "# Get confidence scores (max probability)\n",
    "confidence_scores = np.max(y_val_prob_binary, axis=1)\n",
    "plt.hist(confidence_scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average prediction confidence: {confidence_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and coefficients\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = lr_binary.coef_[0]\n",
    "\n",
    "# Create a dataframe for easier analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 15 features for POSITIVE sentiment:\")\n",
    "positive_features = feature_importance.tail(15)\n",
    "print(positive_features[['feature', 'coefficient']])\n",
    "\n",
    "print(\"\\nTop 15 features for NEGATIVE sentiment:\")\n",
    "negative_features = feature_importance.head(15)\n",
    "print(negative_features[['feature', 'coefficient']])\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = pd.concat([negative_features.head(10), positive_features.tail(10)])\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], \n",
    "         color=['red' if x < 0 else 'green' for x in top_features['coefficient']])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top Features: Negative (Red) vs Positive (Green) Sentiment')\n",
    "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vectorizer):\n",
    "    \"\"\"Test the model on a single review\"\"\"\n",
    "    # Preprocess the text (same as training)\n",
    "    clean_text = preprocess_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vector = vectorizer.transform([clean_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(text_vector)[0]\n",
    "    probability = model.predict_proba(text_vector)[0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test some examples\n",
    "test_reviews = [\n",
    "    \"This product is absolutely amazing! I love it so much!\",\n",
    "    \"Terrible quality, completely broke after one day. Waste of money.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Outstanding customer service and fast shipping. Highly recommend!\",\n",
    "    \"Cheap plastic, looks nothing like the pictures. Very disappointed.\"\n",
    "]\n",
    "\n",
    "print(\"Testing Individual Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "for review in test_reviews:\n",
    "    sentiment, confidence = predict_sentiment(review, lr_binary, vectorizer)\n",
    "    print(f\"Review: {review[:50]}...\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression for Multi-Class Classification (1-5 stars)\")\n",
    "print(\"=\" * 70)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Train multi-class model\n",
    "lr_multi = OneVsRestClassifier(LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0  # One-vs-Rest for multi-class\n",
    "))\n",
    "\n",
    "lr_multi.fit(X_train_vectors, y_train_multi)\n",
    "\n",
    "# Predict\n",
    "y_val_pred_multi = lr_multi.predict(X_val_vectors)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_multi = accuracy_score(y_val_multi, y_val_pred_multi)\n",
    "print(f\"Multi-class Validation Accuracy: {accuracy_multi:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val_multi, y_val_pred_multi))\n",
    "\n",
    "# Confusion matrix for multi-class\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_multi = confusion_matrix(y_val_multi, y_val_pred_multi)\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.title('Multi-Class Confusion Matrix')\n",
    "plt.ylabel('Actual Rating')\n",
    "plt.xlabel('Predicted Rating')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
