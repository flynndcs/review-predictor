{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Environment ready! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the dataset\n",
    "print(\"Loading Amazon Reviews dataset...\")\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \n",
    "                      \"raw_review_All_Beauty\", \n",
    "                      trust_remote_code=True)\n",
    "\n",
    "df = dataset['full'].to_pandas()\n",
    "print(f\"Loaded {len(df)} reviews!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, let's work with a manageable subset for development\n",
    "# Start with 10k samples, scale up later\n",
    "sample_size = min(10000, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Working with {len(df_sample)} samples for initial development\")\n",
    "\n",
    "# Create features and target\n",
    "X = df_sample['text'].copy()\n",
    "y = df_sample['rating'].copy()\n",
    "\n",
    "# Split the data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 â‰ˆ 0.15\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\") \n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "for split_name, split_y in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    dist = split_y.value_counts(normalize=True).sort_index() * 100\n",
    "    print(f\"{split_name}: {dict(dist.round(1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Basic text preprocessing function\n",
    "    Start simple, add complexity as needed\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Optional: Remove URLs, emails, etc.\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "X_train_clean = X_train.apply(preprocess_text)\n",
    "X_val_clean = X_val.apply(preprocess_text)\n",
    "X_test_clean = X_test.apply(preprocess_text)\n",
    "\n",
    "print(\"Sample preprocessed texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {X_train.iloc[i][:100]}...\")\n",
    "    print(f\"Cleaned:  {X_train_clean.iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TF-IDF vectorizer for baseline model\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # Start with 5k features\n",
    "    stop_words='english',   # Remove common words\n",
    "    ngram_range=(1, 2),     # Unigrams and bigrams\n",
    "    min_df=2,               # Ignore terms appearing in < 2 documents\n",
    "    max_df=0.95             # Ignore terms appearing in > 95% of documents\n",
    ")\n",
    "\n",
    "# Fit only on training data\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_vectors = vectorizer.fit_transform(X_train_clean)\n",
    "X_val_vectors = vectorizer.transform(X_val_clean)\n",
    "X_test_vectors = vectorizer.transform(X_test_clean)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_vectors.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Look at some features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Training Random Forest for Multi-Class Classification (1-5 stars)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Multi-class target (1-5 stars)\n",
    "y_train_multi = y_train.copy()\n",
    "y_val_multi = y_val.copy()\n",
    "y_test_multi = y_test.copy()\n",
    "\n",
    "# Initialize Random Forest with good defaults\n",
    "rf_multi = RandomForestClassifier(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=20,           # Prevent overfitting\n",
    "    min_samples_split=10,   # Prevent overfitting  \n",
    "    min_samples_leaf=5,     # Prevent overfitting\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Random Forest...\")\n",
    "rf_multi.fit(X_train_vectors, y_train_multi)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_val_pred_rf = rf_multi.predict(X_val_vectors)\n",
    "y_val_prob_rf = rf_multi.predict_proba(X_val_vectors)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_val_multi, y_val_pred_rf)\n",
    "print(f\"Random Forest Validation Accuracy: {accuracy_rf:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val_multi, y_val_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "cm_rf = confusion_matrix(y_val_multi, y_val_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.ylabel('Actual Rating')\n",
    "plt.xlabel('Predicted Rating')\n",
    "\n",
    "# Prediction Confidence Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "confidence_scores_rf = np.max(y_val_prob_rf, axis=1)\n",
    "plt.hist(confidence_scores_rf, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "plt.title('RF Prediction Confidence')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Class-wise Performance\n",
    "plt.subplot(1, 3, 3)\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = []\n",
    "for class_val in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
    "    mask = y_val_multi == class_val\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = accuracy_score(y_val_multi[mask], y_val_pred_rf[mask])\n",
    "        class_accuracies.append(class_acc)\n",
    "    else:\n",
    "        class_accuracies.append(0)\n",
    "\n",
    "plt.bar([1,2,3,4,5], class_accuracies, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average prediction confidence: {confidence_scores_rf.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest gives us feature importance!\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importance_scores = rf_multi.feature_importances_\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look for interesting patterns\n",
    "print(\"\\nInteresting patterns in top features:\")\n",
    "top_20_features = top_features['feature'].tolist()\n",
    "print(\"Words that might indicate positive sentiment:\", \n",
    "      [word for word in top_20_features if word in ['great', 'amazing', 'excellent', 'love', 'perfect', 'best']])\n",
    "print(\"Words that might indicate negative sentiment:\", \n",
    "      [word for word in top_20_features if word in ['bad', 'terrible', 'awful', 'hate', 'worst', 'horrible']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_rf_prediction(text, model, vectorizer, feature_names):\n",
    "    \"\"\"Explain how Random Forest made a specific prediction\"\"\"\n",
    "    \n",
    "    # Preprocess and vectorize\n",
    "    clean_text = preprocess_text(text)\n",
    "    text_vector = vectorizer.transform([clean_text])\n",
    "    \n",
    "    # Get prediction and probabilities\n",
    "    prediction = model.predict(text_vector)[0]\n",
    "    probabilities = model.predict_proba(text_vector)[0]\n",
    "    \n",
    "    print(f\"Review: {text[:100]}...\")\n",
    "    print(f\"Predicted Rating: {prediction}\")\n",
    "    print(\"Probability distribution:\")\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        stars = i + 1 if model.classes_[i] == i + 1 else model.classes_[i]\n",
    "        print(f\"  {stars} stars: {prob:.3f}\")\n",
    "    \n",
    "    # Find most important features for this prediction\n",
    "    # Get the features that are present in this text\n",
    "    feature_vector = text_vector.toarray()[0]\n",
    "    present_features = []\n",
    "    \n",
    "    for i, value in enumerate(feature_vector):\n",
    "        if value > 0:  # Feature is present\n",
    "            feature_name = feature_names[i]\n",
    "            importance = model.feature_importances_[i]\n",
    "            present_features.append((feature_name, importance, value))\n",
    "    \n",
    "    # Sort by importance\n",
    "    present_features.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop contributing features in this review:\")\n",
    "    for feature, importance, tfidf_value in present_features[:10]:\n",
    "        print(f\"  '{feature}': importance={importance:.4f}, tf-idf={tfidf_value:.3f}\")\n",
    "\n",
    "# Test on some examples\n",
    "test_reviews = [\n",
    "    \"This product is absolutely amazing! I love it so much and would highly recommend it to everyone.\",\n",
    "    \"Terrible quality, completely broke after one day. Waste of money and very disappointed.\",\n",
    "    \"It's okay, nothing special but does the job. Average product overall.\",\n",
    "    \"Great value for money, works as expected and arrived quickly.\",\n",
    "    \"Horrible experience, customer service was rude and product was damaged.\"\n",
    "]\n",
    "\n",
    "print(\"Random Forest Prediction Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for i, review in enumerate(test_reviews):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    explain_rf_prediction(review, rf_multi, vectorizer, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can improve with different hyperparameters\n",
    "print(\"Model Hyperparameters:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Number of trees: {rf_multi.n_estimators}\")\n",
    "print(f\"Max depth: {rf_multi.max_depth}\")\n",
    "print(f\"Min samples split: {rf_multi.min_samples_split}\")\n",
    "print(f\"Min samples leaf: {rf_multi.min_samples_leaf}\")\n",
    "\n",
    "# Quick hyperparameter sensitivity test\n",
    "print(\"\\nQuick hyperparameter test (this might take a moment):\")\n",
    "\n",
    "# Test different number of trees\n",
    "tree_counts = [50, 100, 200]\n",
    "for n_trees in tree_counts:\n",
    "    rf_test = RandomForestClassifier(\n",
    "        n_estimators=n_trees,\n",
    "        max_depth=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_test.fit(X_train_vectors, y_train_multi)\n",
    "    test_acc = rf_test.score(X_val_vectors, y_val_multi)\n",
    "    print(f\"  {n_trees} trees: {test_acc:.3f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
