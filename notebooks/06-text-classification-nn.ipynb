{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification with LSTM and GRU Networks\n",
    "# A Complete Tutorial with Real-World Examples\n",
    "\n",
    "## Overview\n",
    "# This notebook demonstrates text classification using LSTM and GRU networks\n",
    "# We'll use the IMDB movie reviews dataset for sentiment analysis\n",
    "# You'll learn to build, train, and compare both architectures\n",
    "\n",
    "# Required installations (run in terminal):\n",
    "# pip install tensorflow numpy pandas matplotlib seaborn scikit-learn wordcloud\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "# Load IMDB dataset - 50,000 movie reviews (25k train, 25k test)\n",
    "# We'll use top 10,000 most frequent words\n",
    "vocab_size = 10000\n",
    "max_length = 500  # Maximum sequence length\n",
    "\n",
    "print(\"Loading IMDB dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Examine data structure\n",
    "print(f\"\\nFirst review (encoded): {X_train[0][:20]}...\")\n",
    "print(f\"First review label: {y_train[0]} (0=negative, 1=positive)\")\n",
    "print(f\"Review lengths - Min: {min(len(x) for x in X_train)}, Max: {max(len(x) for x in X_train)}\")\n",
    "\n",
    "# Get word index to decode reviews\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "print(f\"\\nDecoded first review: {decode_review(X_train[0])[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "print(\"Padding sequences...\")\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"Training data shape: {X_train_padded.shape}\")\n",
    "print(f\"Testing data shape: {X_test_padded.shape}\")\n",
    "\n",
    "# Visualize sequence lengths\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "lengths = [len(x) for x in X_train]\n",
    "plt.hist(lengths, bins=50, alpha=0.7)\n",
    "plt.axvline(max_length, color='red', linestyle='--', label=f'Max length: {max_length}')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_train, bins=2, alpha=0.7)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Model Architecture Comparison\n",
    "\n",
    "# Define function to create LSTM model\n",
    "def create_lstm_model(vocab_size, embedding_dim=128, lstm_units=64, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define function to create GRU model\n",
    "def create_gru_model(vocab_size, embedding_dim=128, gru_units=64, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        GRU(gru_units, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define function to create Bidirectional LSTM model\n",
    "def create_bilstm_model(vocab_size, embedding_dim=128, lstm_units=64, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        Bidirectional(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_split.shape[0]} samples\")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Train and Compare Models\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "# LSTM Model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training LSTM Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lstm_model = create_lstm_model(vocab_size)\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "models['LSTM'] = lstm_model\n",
    "histories['LSTM'] = history_lstm\n",
    "\n",
    "# GRU Model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training GRU Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "gru_model = create_gru_model(vocab_size)\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(gru_model.summary())\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "models['GRU'] = gru_model\n",
    "histories['GRU'] = history_gru\n",
    "\n",
    "# Bidirectional LSTM Model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Bidirectional LSTM Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "bilstm_model = create_bilstm_model(vocab_size)\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(bilstm_model.summary())\n",
    "\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "models['BiLSTM'] = bilstm_model\n",
    "histories['BiLSTM'] = history_bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Visualization of Training Progress\n",
    "\n",
    "def plot_training_history(histories):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training Loss\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    for name, history in histories.items():\n",
    "        axes[0, 0].plot(history.history['loss'], label=f'{name}')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Validation Loss\n",
    "    axes[0, 1].set_title('Validation Loss')\n",
    "    for name, history in histories.items():\n",
    "        axes[0, 1].plot(history.history['val_loss'], label=f'{name}')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Training Accuracy\n",
    "    axes[1, 0].set_title('Training Accuracy')\n",
    "    for name, history in histories.items():\n",
    "        axes[1, 0].plot(history.history['accuracy'], label=f'{name}')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Validation Accuracy\n",
    "    axes[1, 1].set_title('Validation Accuracy')\n",
    "    for name, history in histories.items():\n",
    "        axes[1, 1].plot(history.history['val_accuracy'], label=f'{name}')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    accuracy = evaluate_model(model, X_test_padded, y_test, name)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Real-World Application Example\n",
    "\n",
    "def predict_sentiment(text, model, word_index, max_length=500):\n",
    "    \"\"\"\n",
    "    Predict sentiment of a custom text input\n",
    "    \"\"\"\n",
    "    # Convert text to sequence\n",
    "    words = text.lower().split()\n",
    "    sequence = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_index and word_index[word] < vocab_size:\n",
    "            sequence.append(word_index[word])\n",
    "    \n",
    "    # Pad sequence\n",
    "    sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sequence, verbose=0)[0][0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test with custom reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
    "    \"Terrible movie. Waste of time. Poor acting and boring storyline.\",\n",
    "    \"The movie was okay, nothing special but not bad either.\",\n",
    "    \"Amazing cinematography and outstanding performances by all actors. Highly recommended!\",\n",
    "    \"Worst movie I've ever seen. Couldn't even finish watching it.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CUSTOM REVIEW PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the best performing model (typically BiLSTM)\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Using {best_model_name} model (accuracy: {results[best_model_name]:.4f})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    sentiment, confidence = predict_sentiment(review, best_model, word_index)\n",
    "    print(f\"Review {i}: {review}\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Key Takeaways and Next Steps\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "1. LSTM vs GRU Performance:\n",
    "   - Both models show similar performance on this task\n",
    "   - GRU is slightly faster to train due to fewer parameters\n",
    "   - LSTM has separate forget and input gates, potentially better for complex sequences\n",
    "\n",
    "2. Bidirectional Models:\n",
    "   - BiLSTM often performs better as it processes sequences in both directions\n",
    "   - Captures context from both past and future words\n",
    "\n",
    "3. Model Architecture Insights:\n",
    "   - Embedding layer converts words to dense vectors\n",
    "   - Dropout prevents overfitting\n",
    "   - Dense layers add classification capacity\n",
    "\n",
    "4. Performance Factors:\n",
    "   - Sequence length impacts performance\n",
    "   - Vocabulary size affects model complexity\n",
    "   - Early stopping prevents overfitting\n",
    "\n",
    "NEXT STEPS FOR IMPROVEMENT:\n",
    "- Try pre-trained embeddings (Word2Vec, GloVe)\n",
    "- Experiment with attention mechanisms\n",
    "- Use larger models or ensemble methods\n",
    "- Try different preprocessing techniques\n",
    "- Implement cross-validation for robust evaluation\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Production-Ready Prediction Function\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, model, word_index, vocab_size=10000, max_length=500):\n",
    "        self.model = model\n",
    "        self.word_index = word_index\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text for prediction\"\"\"\n",
    "        words = text.lower().split()\n",
    "        sequence = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.word_index and self.word_index[word] < self.vocab_size:\n",
    "                sequence.append(self.word_index[word])\n",
    "        \n",
    "        return pad_sequences([sequence], maxlen=self.max_length, padding='post')\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict sentiment with confidence score\"\"\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        prediction = self.model.predict(processed_text, verbose=0)[0][0]\n",
    "        \n",
    "        sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "        confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': float(confidence),\n",
    "            'raw_score': float(prediction)\n",
    "        }\n",
    "    \n",
    "    def batch_predict(self, texts):\n",
    "        \"\"\"Predict sentiment for multiple texts\"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            results.append(self.predict(text))\n",
    "        return results\n",
    "\n",
    "# Create production classifier instance\n",
    "classifier = SentimentClassifier(best_model, word_index)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"This movie exceeded all my expectations. Brilliant storytelling!\"\n",
    "result = classifier.predict(sample_text)\n",
    "print(f\"\\nProduction Classifier Result: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"You now have a complete text classification system using LSTM/GRU networks!\")\n",
    "print(\"The SentimentClassifier class is ready for production use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
